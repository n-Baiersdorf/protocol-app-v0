#!/bin/bash

echo "ğŸ”§ Repariere Protokoll-LLM Setup..."

# Alte Container stoppen und entfernen
echo "â¹ï¸  Stoppe alle Container..."
docker-compose down

# PrÃ¼fe ob Ollama bereits lÃ¤uft
echo "ğŸ” PrÃ¼fe Ollama-Status..."
if curl -s http://localhost:11434/api/version > /dev/null 2>&1; then
    echo "âœ… Ollama lÃ¤uft bereits auf localhost:11434"
    echo "ğŸ“‹ VerfÃ¼gbare Modelle:"
    curl -s http://localhost:11434/api/tags | jq -r '.models[].name' 2>/dev/null || echo "Keine Modelle gefunden"
    echo "â„¹ï¸  Backend wird sich Ã¼ber 172.17.0.1:11434 verbinden"
else
    echo "âŒ Ollama nicht erreichbar auf localhost:11434"
    echo "Starte Ollama mit: ollama serve"
    exit 1
fi

# Nur Backend und Frontend starten (ohne Ollama-Container)
echo "ğŸš€ Starte Backend und Frontend..."
docker-compose up -d backend frontend db

# Warten bis Services bereit sind
echo "â³ Warte auf Services..."
sleep 10

# Gesundheitscheck
echo "ğŸ¥ PrÃ¼fe Service-Status..."
if curl -f http://localhost:5000/health > /dev/null 2>&1; then
    echo "âœ… Backend lÃ¤uft: http://localhost:5000"
else
    echo "âŒ Backend-Problem - prÃ¼fe Logs mit: docker-compose logs backend"
fi

if curl -f http://localhost:3000 > /dev/null 2>&1; then
    echo "âœ… Frontend lÃ¤uft: http://localhost:3000"
else
    echo "âŒ Frontend-Problem - prÃ¼fe Logs mit: docker-compose logs frontend"
fi

# Stelle sicher, dass benÃ¶tigte Modelle vorhanden sind
echo "ğŸ“¥ PrÃ¼fe Ollama-Modelle..."
if ! curl -s http://localhost:11434/api/tags | grep -q "llama2"; then
    echo "â¬‡ï¸  Lade llama2 Modell..."
    ollama pull llama2
fi

echo "âœ… Setup-Reparatur abgeschlossen!"
echo "ğŸŒ Anwendung verfÃ¼gbar unter: http://localhost:3000"
echo "ğŸ”§ API-Dokumentation: http://localhost:5000/health" 